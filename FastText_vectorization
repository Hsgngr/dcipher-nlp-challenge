#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Dec 31 17:29:58 2020

@author: ege
"""
import numpy as np
import pandas as pd
import tensorflow as tf
np.random.seed(0)
from keras.models import Model
from keras.layers import Dense, Input, Dropout, LSTM, Activation
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.initializers import glorot_uniform
np.random.seed(1)

import nltk
import re
from string import punctuation
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk.stem as stemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize
from nltk import WordPunctTokenizer


nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
en_stop = set(nltk.corpus.stopwords.words('english'))

#Import dataset and split to train,validation,test
train = pd.read_json('data/wos2class.train.json')
test  = pd.read_json('data/wos2class.test.json')

train_title = list(train['Title'])
train_abstract = list(train['Abstract'])
train_title.extend(train_abstract)


def preprocess_text(document):
  #Remove  all the special characters
  document = re.sub(r'\W', ' ', str(document))

  #remove all single characters
  document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)

  #Remove single characters from the start
  document = re.sub(r'\^[a-zA-Z]\s+', ' ', document)

  #Substituting multiple spaces with single space
  document = re.sub(r'\s+', ' ', document, flags =re.I)

  #Removing prefixed 'b'
  document = re.sub(r'^b\s+', '', document)

  #Converting to lower_case
  document = document.lower()

  #Lemmatization
  tokens = document.split()
  tokens = [stemmer.lemmatize(word) for word in tokens]
  tokens = [word for word in tokens if word not in en_stop]
  tokens = [word for word in tokens if len(word) > 3]

  preprocessed_text =' '.join(tokens)

  return preprocessed_text

#Test preprocessing of text:
stemmer = WordNetLemmatizer()
sent = preprocess_text('Artificial intelligence, is the most advanced technology of the present era')
print(sent,'\nIt is working!')

def sentences_to_indices(sentence_list,word_to_index,max_len):
    
    """
    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.
    
    Arguments:
    sentence_list -- The column of sentences which is going to be converted
    word_to_index -- a dictionary containing the each word mapped to its index
    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. 
    
    Returns:
    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)
    """
    
    #Normally its string punctuation
    punctuations = '!"#$%&\'()*+,./:;<=>?@[\\]^_`{|}~'
    table_ = str.maketrans('', '', punctuations) #for removing any punctuations
    #Number of samples
    m = len(column)                                  
    #initialize a the array for Title_indices
    X_indices = np.zeros((m,max_len))
    
    for i in range(m):
        
        sentence_without_punc = column[i].translate(table_)                       
        sentence_words = (sentence_without_punc.lower()).split()
        
        #print(sentence_words)
        j = 0
        
        for w in sentence_words:
            # Set the (i,j)th entry of X_indices to the index of the correct word.
            #print(w)
            
            try:
                X_indices[i, j] = word_to_index[w]
            except:
                print('unknown word: ',w)
                X_indices[i, j] = word_to_index['unk']
                unknown_word_counter += 1
                unique_unknown_words.add(w)
                
            finally:
                unique_words.add(w)
                j = j+1           
    
    print('total unique words', len(unique_words))
    print('total unique unknown words', len(unique_unknown_words))
    print('Counter of unknown words: ', unknown_word_counter)
    X_indices = X_indices.tolist()
    return X_indices